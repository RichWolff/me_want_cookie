{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = pathlib.Path(os.getenv('BASE_DIR'))\n",
    "data = base_dir / 'data'\n",
    "data_raw = data / 'raw'\n",
    "images = data_raw / 'images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv(data_raw / 'seasons46to50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "def preprocess(series):\n",
    "    series = series.str.lower()\n",
    "    series = series.str.replace('{html}',\"\") \n",
    "    \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    series = series.str.replace(cleanr, '')\n",
    "    \n",
    "    series = series.str.replace(r'http\\S+', '')\n",
    "    series = series.str.replace('[0-9]+', '')\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    series = series.apply(tokenizer.tokenize)  \n",
    "    series = series.apply(lambda tokens: [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')])\n",
    "    series = series.apply(lambda filtered_words: ' '.join([lemmatizer.lemmatize(w) for w in filtered_words]))\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_summaries = df.groupby('Episode')['Description'].agg(list).apply(lambda segs: ' '.join(segs))\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "cleaned = preprocess(episode_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency distribution\n",
    "from nltk import FreqDist\n",
    "from itertools import chain\n",
    "fdist = FreqDist(' '.join(cleaned).split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fdist = FreqDist(' '.join(cleaned).split(' '\n",
    "fig, ax = plt.subplots(figsize=(16,4),dpi=300)\n",
    "fdist.plot(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A couple of first thoughts:\n",
    "- Monster is a common \"last name\" for the puppets (cookie monster, elmo monster, etc). This may be over used and could be removed\n",
    "- Why is the word \"first\" on in the top 3 list? Could the summaries say \"First, elmo did this\".\n",
    "- The word day may be \"number of the day, letter of the day\" this wont provide much value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_common_ngrams(col: pd.Series, n: int=2):\n",
    "    zippedTuples = col.apply(ngram_generator, n=n)\n",
    "    chainedTuples = chain.from_iterable(zippedTuples)\n",
    "    return Counter(chainedTuples)\n",
    "\n",
    "def ngram_generator(s: str, n: int=2):\n",
    "    wordIdxs = []\n",
    "    for i in range(1, n+1):\n",
    "        wordIdxs.append(s[i:])\n",
    "    return zip(*wordIdxs)\n",
    "\n",
    "\n",
    "count = cleaned.str.split(' ').aggregate(most_common_ngrams, n=2)\n",
    "for text, cnt in count.most_common(10):\n",
    "    print(cnt,'-',' '.join(text))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the top 10 2 gram words, we can probably remove a few common words to every episode\n",
    "- First Episode (not much added info here)\n",
    "- letter day (in every episode)\n",
    "- number day (in every episode)\n",
    "- introduce letter (letter of the day related)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeCommon = [\n",
    "    'first episode',\n",
    "    'letter day',\n",
    "    'number day',\n",
    "    'introduce letter'\n",
    "]\n",
    "pattern = '|'.join(removeCommon)\n",
    "cleaned2 = cleaned.str.replace(pattern, '')\n",
    "\n",
    "cleaned2 = cleaned2.str.split().apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(' '.join(cleaned2).split(' '))\n",
    "fig, ax = plt.subplots(figsize=(16,4),dpi=300)\n",
    "fdist.plot(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = cleaned2.apply(lambda x: Counter(x.split()))\n",
    "episodeNorm = counter.apply(lambda x: x['grover'])\n",
    "episodeNorm.sort_values(ascending=False, inplace=True)\n",
    "episodeNorm.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEpisodesImages(episodeNumbers):\n",
    "    episodes = pd.read_csv(data_raw / 'seasons46to50.csv')\n",
    "    returnEpisodes = episodes[episodes['Episode'].isin(episodeNumbers)]\n",
    "    returnEpisodes = returnEpisodes.groupby(['Episode']).agg({'Segment':list,'Description':list}).to_dict(orient='index')\n",
    "    return returnEpisodes\n",
    "\n",
    "a = getEpisodesImages([4702])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(cleaned2.str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in cleaned2.str.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 (me_want_cookie)",
   "language": "python",
   "name": "me_want_cookie"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
